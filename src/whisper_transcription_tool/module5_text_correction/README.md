# BatchProcessor: Intelligent Text Chunking for LLM Correction

## Overview

The `BatchProcessor` is a sophisticated text chunking system designed to intelligently split large texts into manageable chunks for LLM processing while maintaining context continuity through sentence-aware overlaps.

## Key Features

### ðŸ§  Intelligent Text Chunking
- **Sentence-boundary respect**: Never splits in the middle of sentences
- **Configurable chunk sizes**: Adjustable max context length (default: 2048 tokens)
- **Smart overlap handling**: Configurable sentence-level overlaps between chunks
- **Multiple tokenization strategies**: SentencePiece, NLTK, Transformers, and simple fallbacks

### âš¡ Processing Modes
- **Asynchronous processing**: Parallel chunk processing with ThreadPoolExecutor
- **Synchronous processing**: Sequential processing for CLI contexts
- **Real-time progress reporting**: Chunk-level progress callbacks
- **Comprehensive error handling**: Continue processing even when individual chunks fail

### ðŸ›¡ï¸ Robust Error Handling
- **Chunk-level error isolation**: One failed chunk doesn't stop the entire process
- **Graceful fallbacks**: Original text preserved when correction fails
- **Detailed error reporting**: Specific error information with chunk positions
- **Partial correction support**: Mixed results with both corrected and original text

## Core Components

### TextChunk Dataclass

```python
@dataclass
class TextChunk:
    text: str              # The actual text content
    index: int             # Sequential chunk index (0-based)
    start_pos: int         # Character position in original text
    end_pos: int           # Character position in original text
    overlap_start: int     # Characters overlapping with previous chunk
    overlap_end: int       # Characters overlapping with next chunk
    sentence_start: int    # First sentence index in chunk
    sentence_end: int      # Last sentence index in chunk
    token_count: int       # Estimated token count
```

### TokenizerStrategy Enum

- `SENTENCEPIECE`: Use SentencePiece tokenizer (for LeoLM compatibility)
- `TRANSFORMERS`: Use Hugging Face transformers tokenizer
- `NLTK`: Use NLTK sentence segmentation
- `SIMPLE`: Use regex-based sentence splitting

### ChunkProcessingResult Dataclass

```python
@dataclass
class ChunkProcessingResult:
    chunk: TextChunk           # Original chunk information
    corrected_text: str        # Processed/corrected text
    processing_time: float     # Time spent processing this chunk
    error: Optional[Exception] # Any error that occurred
    success: bool              # Whether processing succeeded
```

## Usage Examples

### Basic Usage

```python
from whisper_transcription_tool.module5_text_correction import BatchProcessor

# Initialize processor
processor = BatchProcessor(
    max_context_length=2048,  # Max tokens per chunk
    overlap_sentences=1,      # Overlap 1 sentence between chunks
    tokenizer_strategy="nltk" # Tokenization strategy
)

# Chunk text
text = "Your long text here..."
chunks = processor.chunk_text(text)

# Define correction function
def correct_text(text: str) -> str:
    # Your LLM correction logic here
    return corrected_text

# Process synchronously
result = processor.process_chunks_sync(chunks, correct_text)
```

### Asynchronous Processing

```python
import asyncio

async def process_text_async():
    processor = BatchProcessor(max_context_length=1024)

    chunks = processor.chunk_text(your_text)

    # Progress callback
    def progress(current, total, status):
        print(f"Progress: {current}/{total} - {status}")

    result = await processor.process_chunks_async(
        chunks,
        correction_function,
        progress_callback=progress
    )

    return result

# Run async processing
result = asyncio.run(process_text_async())
```

### Error Handling Example

```python
def robust_correction_function(text: str) -> str:
    try:
        # Your LLM correction logic
        return llm_correct(text)
    except Exception as e:
        # Log error but don't raise - let BatchProcessor handle it
        logger.error(f"Correction failed: {e}")
        raise  # BatchProcessor will catch and handle

# Process with automatic error recovery
result = processor.process_chunks_sync(
    chunks,
    robust_correction_function
)
# Failed chunks will contain original text, successful ones corrected text
```

## Advanced Configuration

### Custom Tokenization

```python
# Use specific tokenizer strategy
processor = BatchProcessor(tokenizer_strategy="sentencepiece")

# The processor will automatically fall back to available alternatives
# if the preferred strategy is not available
```

### Memory and Performance Optimization

```python
# For large texts, use smaller chunks and async processing
processor = BatchProcessor(
    max_context_length=1024,  # Smaller chunks
    overlap_sentences=1       # Minimal overlap
)

# Async processing with limited concurrency
result = await processor.process_chunks_async(
    chunks,
    correction_fn,
    # BatchProcessor internally limits to 4 concurrent workers
)
```

### Custom Progress Reporting

```python
def detailed_progress(current: int, total: int, status: str):
    percentage = (current / total) * 100
    estimated_remaining = processor.estimate_processing_time(
        remaining_text, chars_per_second=50
    )

    print(f"[{percentage:5.1f}%] {current}/{total}")
    print(f"Status: {status}")
    print(f"ETA: {estimated_remaining}s")

processor.process_chunks_sync(chunks, correction_fn, detailed_progress)
```

## Integration with Whisper Transcription Tool

The BatchProcessor is designed to integrate seamlessly with the existing transcription workflow:

```python
# In transcription workflow
from whisper_transcription_tool.module5_text_correction import BatchProcessor

async def correct_transcription(transcription_text: str) -> str:
    processor = BatchProcessor()
    chunks = processor.chunk_text(transcription_text)

    corrected = await processor.process_chunks_async(
        chunks,
        llm_correction_function,
        websocket_progress_callback  # For real-time UI updates
    )

    return corrected
```

## Performance Characteristics

### Chunking Performance
- **Small texts** (< 1000 chars): Negligible overhead
- **Medium texts** (1000-10000 chars): < 100ms chunking time
- **Large texts** (> 10000 chars): Linear scaling with text length

### Processing Performance
- **Sync processing**: Sequential, predictable timing
- **Async processing**: Up to 4x speedup with 4 concurrent workers
- **Memory usage**: Minimal overhead, chunks processed independently

### Error Recovery
- **Failed chunk handling**: Continues processing remaining chunks
- **Partial results**: Returns mixed corrected/original text
- **Error reporting**: Detailed information about failures

## Dependencies

### Required
- Python 3.8+
- `asyncio` (standard library)
- `concurrent.futures` (standard library)
- `dataclasses` (standard library)
- `typing` (standard library)
- `re` (standard library)
- `logging` (standard library)

### Optional (with fallbacks)
- `sentencepiece`: For LeoLM-compatible tokenization
- `nltk`: For robust sentence segmentation
- `transformers`: For Hugging Face tokenizer support

## Testing

Run the examples to test all functionality:

```bash
python -m whisper_transcription_tool.module5_text_correction.examples
```

## Implementation Notes

### Sentence Boundary Detection
The processor uses multiple strategies for sentence detection:
1. **NLTK punkt tokenizer** (preferred for accuracy)
2. **Regex-based splitting** (fallback for German text)
3. **Simple punctuation matching** (ultimate fallback)

### Token Estimation
Token counting uses the best available method:
1. **SentencePiece tokenizer** (most accurate for LeoLM)
2. **Transformers tokenizer** (good approximation)
3. **Word-based estimation** (1.3x words for German)

### Overlap Handling
Overlaps are calculated at the sentence level and intelligently merged:
- **Forward overlap**: Sentences from previous chunk included in current
- **Backward overlap**: Sentences from current chunk included in next
- **Merge strategy**: Remove duplicate content during reassembly

## Error Scenarios and Recovery

### Common Error Patterns
1. **LLM timeout/failure**: Chunk reverts to original text
2. **Memory errors**: Processing continues with remaining chunks
3. **Network issues**: Isolated to specific chunks
4. **Malformed input**: Validation prevents processing invalid chunks

### Recovery Strategies
1. **Graceful degradation**: Always return usable text
2. **Partial success reporting**: Clear indication of what succeeded/failed
3. **Detailed logging**: Full error context for debugging
4. **Automatic fallbacks**: Multiple levels of backup strategies

## Future Enhancements

- **Dynamic chunk sizing**: Adjust chunk size based on content complexity
- **Context-aware overlaps**: Smarter overlap calculation based on semantic similarity
- **Streaming processing**: Process chunks as they're generated
- **Multi-language support**: Extended tokenization for other languages
- **Performance metrics**: Built-in timing and throughput measurement

---

# ResourceManager: Advanced Resource Management for LLM Operations

## Overview

The **ResourceManager** is a comprehensive resource management system that provides thread-safe singleton resource management for AI model operations. It handles memory monitoring, model lifecycle management, and performance optimization for both Whisper and LeoLM models.

## Key Features

### ðŸ”’ Thread-Safe Singleton Pattern
- Double-checked locking implementation
- Thread-safe initialization
- Guaranteed single instance across all threads

### ðŸ’¾ Memory Management
- Real-time memory monitoring with `psutil`
- Configurable memory thresholds (warning: 80%, critical: 90%)
- Automatic garbage collection and cleanup
- Memory constraint checking before model loading

### ðŸ”„ Model Lifecycle Management
- Support for Whisper (subprocess) and LeoLM (llama-cpp-python) models
- Thread-safe model loading/unloading with exclusive locks
- Model swapping with proper cleanup delays (2.5 seconds)
- Resource tracking with memory usage and timing metrics

### ðŸ–¥ï¸ GPU Acceleration Detection
- **macOS**: Metal Performance Shaders detection
- **Linux**: NVIDIA CUDA detection
- **Fallback**: CPU-only processing

### ðŸ“Š Performance Monitoring
- Optional metrics collection
- Continuous background monitoring thread
- Load/unload timing statistics
- Memory usage tracking (current, peak)
- Model swap performance metrics

### ðŸ›¡ï¸ Error Recovery
- Graceful handling of insufficient memory
- Subprocess termination with timeouts
- Force cleanup capabilities
- Comprehensive status reporting

## Resource Constraints

| Model Type | Min Memory | Preferred Memory | Max Concurrent |
|------------|------------|------------------|----------------|
| **Whisper** | 2.0 GB | 4.0 GB | 2 |
| **LeoLM** | 6.0 GB | 8.0 GB | 1 |

## Usage Examples

### Basic Usage

```python
from whisper_transcription_tool.module5_text_correction import ResourceManager

# Get singleton instance
rm = ResourceManager()

# Enable performance monitoring
rm.enable_monitoring(continuous=True)

# Check system status
status = rm.get_status()
print(f"Memory safe: {status['memory_safe']}")
print(f"GPU acceleration: {status['gpu_acceleration']}")

# Load a model
success = rm.request_model(ModelType.WHISPER, {"model": "large-v3-turbo"})
if success:
    print("Whisper model loaded successfully")
    # Use the model...
    rm.release_model(ModelType.WHISPER)
```

### Model Swapping

```python
# Load initial model
rm.request_model(ModelType.WHISPER, config)

# Swap to LeoLM for text correction
success = rm.swap_models(
    from_model=ModelType.WHISPER,
    to_model=ModelType.LEOLM,
    config={"model_path": "models/leolm-7b.gguf", "n_gpu_layers": 35}
)

if success:
    print("Successfully swapped to LeoLM")
```

### Memory Monitoring

```python
# Check available memory
memory_info = rm.check_available_memory()
print(f"Available: {memory_info['available_gb']:.1f}GB")
print(f"Used: {memory_info['percent_used']:.1f}%")

# Force cleanup if needed
if memory_info['percent_used'] > 85:
    rm.force_cleanup()
```

### Performance Metrics

```python
# Enable monitoring
rm.enable_monitoring()

# Perform operations...

# Get metrics
metrics = rm.get_metrics()
print(f"Model loads: {metrics['model_loads']}")
print(f"Average load time: {metrics['average_load_time']:.3f}s")
print(f"Peak memory usage: {metrics['peak_memory_usage']:.1f}GB")
```

## Testing & Demo

### Running Tests

```bash
# Basic functionality test
cd /path/to/whisper_clean
source venv_new/bin/activate
python -m pytest src/whisper_transcription_tool/module5_text_correction/test_resource_manager.py -v

# Thread safety tests
python -m pytest src/whisper_transcription_tool/module5_text_correction/test_thread_safety.py -v
```

### Demo Script

```bash
# Run comprehensive demonstration
python src/whisper_transcription_tool/module5_text_correction/demo_resource_manager.py
```

## Performance Benchmarks

Based on demo runs on macOS with 64GB RAM:

- **Singleton Creation**: ~0.005s for 10 concurrent instances
- **Model Swapping**: ~2.5s (includes mandatory cleanup delay)
- **Memory Monitoring**: Real-time with ~10s continuous intervals
- **Thread Safety**: 100% success rate under concurrent load
- **Cleanup Operations**: <0.1s for force garbage collection

## System Requirements

- **Python**: 3.8+
- **Memory**: Minimum 8GB RAM (16GB recommended for LeoLM)
- **Dependencies**:
  - `psutil>=5.9.0` (memory monitoring)
  - `llama-cpp-python>=0.2.0` (LeoLM support, optional)

## Architecture Notes

### Model Management Strategy
1. **Whisper Models**: Run as subprocess (`subprocess.Popen`)
   - Advantages: Process isolation, easy termination
   - Cleanup: SIGTERM â†’ SIGKILL fallback

2. **LeoLM Models**: Run in-process (`llama_cpp.Llama`)
   - Advantages: Direct memory management, GPU acceleration
   - Cleanup: Destructor call + forced garbage collection

### Thread Safety Implementation
- **Double-checked locking** for singleton initialization
- **Per-model locks** (`threading.Lock`) for exclusive access
- **Metrics lock** for thread-safe performance data
- **Atomic operations** for status updates

### Memory Management Philosophy
- **Proactive monitoring** with configurable thresholds
- **Graceful degradation** when memory is limited
- **Forced cleanup** as last resort before failures
- **Resource constraints** enforced before model loading

## Integration Examples

### With BatchProcessor

```python
# Combined text correction workflow
from whisper_transcription_tool.module5_text_correction import (
    ResourceManager, BatchProcessor, ModelType
)

async def correct_large_text(text: str) -> str:
    # Initialize resource management
    rm = ResourceManager()
    rm.enable_monitoring()

    # Load LeoLM model
    if not rm.request_model(ModelType.LEOLM, {"n_gpu_layers": 35}):
        raise RuntimeError("Failed to load LeoLM model")

    try:
        # Initialize batch processor
        processor = BatchProcessor(max_context_length=2048)
        chunks = processor.chunk_text(text)

        # Process with LLM correction
        corrected = await processor.process_chunks_async(
            chunks, llm_correction_function
        )

        return corrected

    finally:
        # Always clean up resources
        rm.release_model(ModelType.LEOLM)
        rm.force_cleanup()
```

### With Web Interface

```python
# API endpoint for correction with resource checking
from fastapi import HTTPException

@app.post("/correct-text")
async def correct_text_endpoint(request: CorrectionRequest):
    rm = ResourceManager()

    # Check system resources
    status = rm.get_status()
    if not status['memory_safe']:
        raise HTTPException(
            status_code=503,
            detail=f"System resources insufficient: {status['memory_warning']}"
        )

    # Proceed with correction...
```

---

**Author**: ResourceSentinel Agent
**Version**: 1.0.0
**Tasks Completed**: 2.1, 2.2, 2.3 (Full ResourceManager Implementation)
**Status**: âœ… Production Ready