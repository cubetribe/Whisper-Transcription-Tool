#!/usr/bin/env python3
"""
Comprehensive unit tests for module5_text_correction.

Tests core functionality without requiring external dependencies.
"""

import unittest
import sys
import os
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class TestCorrectionModels(unittest.TestCase):
    """Test core data models."""

    def setUp(self):
        """Set up test fixtures."""
        # Import models module directly to avoid dependency issues
        sys.path.append(str(project_root / "src" / "whisper_transcription_tool" / "module5_text_correction"))
        import models
        self.models = models

    def test_correction_level_enum(self):
        """Test CorrectionLevel enum values."""
        levels = list(self.models.CorrectionLevel)
        self.assertEqual(len(levels), 3)
        self.assertIn(self.models.CorrectionLevel.LIGHT, levels)
        self.assertIn(self.models.CorrectionLevel.STANDARD, levels)
        self.assertIn(self.models.CorrectionLevel.STRICT, levels)

        # Test string values
        self.assertEqual(self.models.CorrectionLevel.LIGHT.value, "light")
        self.assertEqual(self.models.CorrectionLevel.STANDARD.value, "standard")
        self.assertEqual(self.models.CorrectionLevel.STRICT.value, "strict")

    def test_correction_result_creation(self):
        """Test CorrectionResult dataclass creation and methods."""
        result = self.models.CorrectionResult(
            original_text="Test text with erors.",
            corrected_text="Test text with errors.",
            success=True,
            correction_level="standard",
            processing_time_seconds=1.5,
            chunks_processed=1,
            model_used="test-model",
            error_message=None,
            metadata={"test": "data"}
        )

        self.assertEqual(result.original_text, "Test text with erors.")
        self.assertEqual(result.corrected_text, "Test text with errors.")
        self.assertTrue(result.success)
        self.assertEqual(result.correction_level, "standard")
        self.assertEqual(result.processing_time_seconds, 1.5)
        self.assertEqual(result.chunks_processed, 1)
        self.assertEqual(result.model_used, "test-model")
        self.assertIsNone(result.error_message)
        self.assertEqual(result.metadata, {"test": "data"})

    def test_correction_result_to_dict(self):
        """Test CorrectionResult serialization."""
        result = self.models.CorrectionResult(
            original_text="Test",
            corrected_text="Test corrected",
            success=True,
            correction_level="light",
            processing_time_seconds=0.5,
            chunks_processed=1,
            model_used="test"
        )

        result_dict = result.to_dict()

        self.assertIsInstance(result_dict, dict)
        self.assertEqual(result_dict["original_text"], "Test")
        self.assertEqual(result_dict["corrected_text"], "Test corrected")
        self.assertEqual(result_dict["success"], True)
        self.assertEqual(result_dict["correction_level"], "light")
        self.assertEqual(result_dict["processing_time_seconds"], 0.5)
        self.assertEqual(result_dict["chunks_processed"], 1)
        self.assertEqual(result_dict["model_used"], "test")
        self.assertEqual(result_dict["metadata"], {})

    def test_text_chunk_creation(self):
        """Test TextChunk dataclass creation and methods."""
        chunk = self.models.TextChunk(
            text="This is a test chunk.",
            index=0,
            start_pos=0,
            end_pos=21,
            overlap_start=0,
            overlap_end=5
        )

        self.assertEqual(chunk.text, "This is a test chunk.")
        self.assertEqual(chunk.index, 0)
        self.assertEqual(chunk.start_pos, 0)
        self.assertEqual(chunk.end_pos, 21)
        self.assertEqual(chunk.overlap_start, 0)
        self.assertEqual(chunk.overlap_end, 5)

        # Test __len__ method
        self.assertEqual(len(chunk), 21)

    def test_text_chunk_to_dict(self):
        """Test TextChunk serialization."""
        chunk = self.models.TextChunk(
            text="Test chunk",
            index=1,
            start_pos=10,
            end_pos=20
        )

        chunk_dict = chunk.to_dict()

        self.assertIsInstance(chunk_dict, dict)
        self.assertEqual(chunk_dict["text"], "Test chunk")
        self.assertEqual(chunk_dict["index"], 1)
        self.assertEqual(chunk_dict["start_pos"], 10)
        self.assertEqual(chunk_dict["end_pos"], 20)
        self.assertEqual(chunk_dict["overlap_start"], 0)
        self.assertEqual(chunk_dict["overlap_end"], 0)

    def test_correction_config_defaults(self):
        """Test CorrectionConfig default values."""
        config = self.models.CorrectionConfig()

        self.assertFalse(config.enabled)
        self.assertEqual(config.context_length, 2048)
        self.assertEqual(config.temperature, 0.3)
        self.assertEqual(config.correction_level, self.models.CorrectionLevel.STANDARD)
        self.assertTrue(config.keep_original)
        self.assertTrue(config.auto_batch)
        self.assertEqual(config.max_parallel_jobs, 1)
        self.assertFalse(config.dialect_normalization)
        self.assertEqual(config.chunk_overlap_sentences, 1)
        self.assertEqual(config.memory_threshold_gb, 6.0)
        self.assertFalse(config.monitoring_enabled)
        self.assertEqual(config.gpu_acceleration, "auto")
        self.assertTrue(config.fallback_on_error)

    def test_correction_config_to_dict(self):
        """Test CorrectionConfig serialization."""
        config = self.models.CorrectionConfig()
        config_dict = config.to_dict()

        self.assertIsInstance(config_dict, dict)
        self.assertIn("enabled", config_dict)
        self.assertIn("correction_level", config_dict)
        self.assertIn("platform_optimization", config_dict)
        self.assertEqual(config_dict["correction_level"], "standard")


class TestCorrectionPrompts(unittest.TestCase):
    """Test correction prompts functionality."""

    def setUp(self):
        """Set up test fixtures."""
        sys.path.append(str(project_root / "src" / "whisper_transcription_tool" / "module5_text_correction"))
        import models
        import correction_prompts
        self.models = models
        self.prompts_module = correction_prompts

    def test_prompt_templates_structure(self):
        """Test PromptTemplates class structure."""
        templates = self.prompts_module.PromptTemplates()

        # Test system prompts exist for all levels
        for level in ["light", "standard", "strict"]:
            self.assertIn(level, templates.SYSTEM_PROMPTS)
            self.assertIsInstance(templates.SYSTEM_PROMPTS[level], str)
            self.assertGreater(len(templates.SYSTEM_PROMPTS[level]), 100)

        # Test user prompts exist
        self.assertIn("correction", templates.USER_PROMPTS)
        self.assertIn("dialect_normalization", templates.USER_PROMPTS)

    def test_get_available_levels(self):
        """Test get_available_levels function."""
        levels = self.prompts_module.get_available_levels()
        self.assertIsInstance(levels, list)
        self.assertEqual(len(levels), 3)
        self.assertIn("light", levels)
        self.assertIn("standard", levels)
        self.assertIn("strict", levels)

    def test_get_correction_prompt_basic(self):
        """Test basic prompt generation."""
        prompt = self.prompts_module.get_correction_prompt(
            "standard",
            "Das ist ein Test mit Felern."
        )

        self.assertIsInstance(prompt, dict)
        self.assertIn("system", prompt)
        self.assertIn("user", prompt)
        self.assertIsInstance(prompt["system"], str)
        self.assertIsInstance(prompt["user"], str)
        self.assertIn("Das ist ein Test mit Felern.", prompt["user"])

    def test_get_correction_prompt_dialect_mode(self):
        """Test prompt generation with dialect normalization."""
        prompt = self.prompts_module.get_correction_prompt(
            "standard",
            "I mog des ned",
            dialect_mode=True
        )

        self.assertIn("system", prompt)
        self.assertIn("user", prompt)
        self.assertIn("I mog des ned", prompt["user"])
        # Should use dialect normalization template
        self.assertIn("Wandle", prompt["user"])

    def test_get_correction_prompt_with_context(self):
        """Test prompt generation with context."""
        prompt = self.prompts_module.get_correction_prompt(
            "standard",
            "Das ist der mittlere Text.",
            prev_context="Das ist der vorherige Text.",
            next_context="Das ist der nachfolgende Text."
        )

        self.assertIn("system", prompt)
        self.assertIn("user", prompt)
        self.assertIn("Das ist der mittlere Text.", prompt["user"])
        self.assertIn("Das ist der vorherige Text.", prompt["user"])
        self.assertIn("Das ist der nachfolgende Text.", prompt["user"])

    def test_validate_prompt_inputs(self):
        """Test input validation."""
        # Valid inputs should not raise
        self.prompts_module.validate_prompt_inputs("standard", "Test text")

        # Invalid level should raise
        with self.assertRaises(ValueError):
            self.prompts_module.validate_prompt_inputs("invalid", "Test text")

        # Empty text should raise
        with self.assertRaises(ValueError):
            self.prompts_module.validate_prompt_inputs("standard", "")

        # Too long text should raise
        with self.assertRaises(ValueError):
            self.prompts_module.validate_prompt_inputs("standard", "x" * 20000)

    def test_correction_prompts_class(self):
        """Test CorrectionPrompts high-level interface."""
        prompts = self.prompts_module.CorrectionPrompts()

        # Test get_available_levels returns enums
        levels = prompts.get_available_levels()
        self.assertIsInstance(levels, list)
        self.assertEqual(len(levels), 3)
        self.assertIsInstance(levels[0], self.models.CorrectionLevel)

        # Test get_prompt with enum
        prompt = prompts.get_prompt(
            self.models.CorrectionLevel.STANDARD,
            "Test text"
        )
        self.assertIsInstance(prompt, dict)
        self.assertIn("system", prompt)
        self.assertIn("user", prompt)

    def test_estimate_tokens(self):
        """Test token estimation."""
        prompts = self.prompts_module.CorrectionPrompts()

        tokens = prompts.estimate_tokens("Test text", self.models.CorrectionLevel.STANDARD)
        self.assertIsInstance(tokens, int)
        self.assertGreater(tokens, 0)

        # Longer text should have more tokens
        long_tokens = prompts.estimate_tokens("This is a much longer text" * 10, self.models.CorrectionLevel.STANDARD)
        self.assertGreater(long_tokens, tokens)

    def test_correction_examples(self):
        """Test correction examples."""
        examples = self.prompts_module.get_correction_examples("standard")
        self.assertIsInstance(examples, dict)
        self.assertIn("input", examples)
        self.assertIn("output", examples)

        # Test all levels have examples
        for level in ["light", "standard", "strict"]:
            examples = self.prompts_module.get_correction_examples(level)
            self.assertIsInstance(examples, dict)

    def test_dialect_examples(self):
        """Test dialect examples."""
        dialect_examples = self.prompts_module.get_dialect_examples()
        self.assertIsInstance(dialect_examples, dict)
        self.assertGreater(len(dialect_examples), 0)

        # Each dialect should have input and output
        for dialect_name, example in dialect_examples.items():
            self.assertIn("input", example)
            self.assertIn("output", example)
            self.assertIsInstance(example["input"], str)
            self.assertIsInstance(example["output"], str)


class TestModuleIntegration(unittest.TestCase):
    """Test module integration and fallback mechanisms."""

    @patch('sys.modules', spec_set=True)
    def test_graceful_import_fallbacks(self):
        """Test that the module handles missing dependencies gracefully."""
        # This is a conceptual test - in reality, we'd need to mock the imports
        # The actual implementation already handles graceful fallbacks
        self.assertTrue(True)  # Placeholder for integration test

    def test_fallback_correction_logic(self):
        """Test rule-based correction fallback."""
        # Test basic corrections that should work without LLM
        test_cases = [
            ("Das ist ein Test √§h mit √§hem F√ºllw√∂rtern.", "Das ist ein Test mit F√ºllw√∂rtern."),
            ("Doppelte  Leerzeichen  Test.", "Doppelte Leerzeichen Test."),
            ("test ohne gro√übuchstabe.", "Test ohne Gro√übuchstabe."),
        ]

        # This would test the fallback logic in the main module
        # For now, we're testing the concept
        for original, expected_pattern in test_cases:
            # Rule-based corrections should handle basic cases
            self.assertNotEqual(original, expected_pattern)

    def test_module_availability_detection(self):
        """Test detection of available vs unavailable modules."""
        # Test that the module can detect which components are available
        # This validates the graceful degradation approach

        # Core models should always be available
        try:
            sys.path.append(str(project_root / "src" / "whisper_transcription_tool" / "module5_text_correction"))
            import models
            self.assertTrue(True)  # Models should always import
        except ImportError:
            self.fail("Core models should always be importable")

        # Optional components might not be available
        optional_components = ["psutil", "llama_cpp", "sentencepiece", "nltk"]
        for component in optional_components:
            try:
                __import__(component)
                # Component is available
                print(f"‚úì {component} available")
            except ImportError:
                # Component not available - this is expected
                print(f"‚ö† {component} not available")


class TestComponentValidation(unittest.TestCase):
    """Validate that all components meet design specifications."""

    def test_dataclass_serialization(self):
        """Test that all dataclasses can be serialized."""
        sys.path.append(str(project_root / "src" / "whisper_transcription_tool" / "module5_text_correction"))
        import models

        # Test all main dataclasses have to_dict methods
        dataclasses_to_test = [
            models.CorrectionResult,
            models.CorrectionJob,
            models.ModelStatus,
            models.SystemResources,
            models.TextChunk,
            models.CorrectionConfig,
        ]

        for dataclass_type in dataclasses_to_test:
            # Each dataclass should have a to_dict method
            self.assertTrue(hasattr(dataclass_type, 'to_dict'),
                          f"{dataclass_type.__name__} should have to_dict method")

    def test_enum_completeness(self):
        """Test that all enums have the expected values."""
        sys.path.append(str(project_root / "src" / "whisper_transcription_tool" / "module5_text_correction"))
        import models

        # Test CorrectionLevel has all required levels
        levels = [level.value for level in models.CorrectionLevel]
        expected_levels = ["light", "standard", "strict"]
        self.assertEqual(set(levels), set(expected_levels))

        # Test ModelType if available
        if hasattr(models, 'ModelType'):
            types = [type.value for type in models.ModelType]
            expected_types = ["whisper", "leolm"]
            self.assertEqual(set(types), set(expected_types))

    def test_prompt_template_completeness(self):
        """Test that all prompt templates are properly defined."""
        sys.path.append(str(project_root / "src" / "whisper_transcription_tool" / "module5_text_correction"))
        import correction_prompts

        templates = correction_prompts.PromptTemplates()

        # All correction levels should have system prompts
        for level in ["light", "standard", "strict"]:
            self.assertIn(level, templates.SYSTEM_PROMPTS)
            prompt = templates.SYSTEM_PROMPTS[level]
            self.assertIsInstance(prompt, str)
            self.assertGreater(len(prompt), 50)  # Should be substantial prompts

        # All user prompt types should exist
        for prompt_type in ["correction", "dialect_normalization", "with_context"]:
            self.assertIn(prompt_type, templates.USER_PROMPTS)
            self.assertIsInstance(templates.USER_PROMPTS[prompt_type], str)


def create_test_suite():
    """Create a test suite with all test classes."""
    suite = unittest.TestSuite()

    # Add all test classes
    test_classes = [
        TestCorrectionModels,
        TestCorrectionPrompts,
        TestModuleIntegration,
        TestComponentValidation,
    ]

    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        suite.addTests(tests)

    return suite


def main():
    """Run all tests."""
    # Create and run test suite
    suite = create_test_suite()
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    # Print summary
    print(f"\n{'='*60}")
    print("TEST SUMMARY")
    print(f"{'='*60}")
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    print(f"Skipped: {len(result.skipped) if hasattr(result, 'skipped') else 0}")

    if result.wasSuccessful():
        print("\nüéâ All tests PASSED!")
        print("Module5 text correction core components are working correctly.")
    else:
        print(f"\n‚ùå {len(result.failures + result.errors)} test(s) FAILED!")
        if result.failures:
            print("\nFailures:")
            for test, traceback in result.failures:
                print(f"  - {test}")
        if result.errors:
            print("\nErrors:")
            for test, traceback in result.errors:
                print(f"  - {test}")

    return result.wasSuccessful()


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)